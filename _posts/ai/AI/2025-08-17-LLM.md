---
layout: post
related_posts:
  - /ai
title:  "Large Language Models(LLM)"
date:   2025-08-17
categories:
  - ai
description: >
  Large Language Model(LLM) 입문
---
* toc
{:toc .large-only}

# Large Language Models(LLM)이란?
대규모 언어 모델(LLM, Large Language Model)이란 **주어진 텍스트에서 다음에 올 가장 확률이 높은 단어를 예측하는 모델**이다.    
이 과정은 모델이 가진 수많은 **파라미터(parameter, weight)** 값에 의해 결정된다.

처음에는 랜덤하게 설정된 파라미터가 학습을 거치며 점점 더 자연스러운 값을 얻게 된다.
특히 **역전파(Backpropagation)** 알고리즘을 통해 오차를 줄이고, 더 정확한 예측을 할 수 있도록 훈련된다.

## Traning 과정
- Step 1: Pretraining(사전훈련)  
대규모 텍스트 데이터를 활용하여 언어패턴을 학습한다.

- Step 2: RLHF(Reinforcement Learning with Human Feedback, 강화학습)    
사람이 모델에 잘못된 응답을 수정하거나 더 적절한 답변을 선택해주는 방식을 사용한다.

이 과정에서 엄청난 파라미터를 병렬로 계산해야 하기 때문에 **GPU**와 같은 고성능 연산 장치가 필수적이다.
단, 모든 언어들이 병렬처리가 최적화된 것은 아니다.


## Transformer
구글 팀에서 개발한 **트랜스포머(Transformer)** 구조는 LLM의 핵심이다.   
기존 RNN/LSTM은 텍스트를 순차적으로 처리했지만, 트랜스포머는 문장을 **병렬 처리**힐 수 있다.
텍스트를 처음부터 끝까지 순차적으로 읽은 다음 전체적으로 병렬로 처리하는 방식이다.
ai는 연속적인 수치데이터에서 이루어질 수 있기 때문에 언어를 숫자로 인코딩 한다.
언어를 숫자로 인코딩하면 각 단어는 벡터(숫자 패턴)로 표현되고, 이 벡터는 단어의 의미나 맥락을 담고 있다.

### Attention
트랜스포머의 핵심 알고리즘은 **Attention**이다.
Attention이라는 강력한 연산 알고리즘을 사용하여 이 숫자 패턴을 처리함
단어 벡터들이 서로 정보를 주고받으며, 문맥에 따라 각 단어의 의미를 적절히 조정할 수 있게 해준다.
즉, 어떤 단어를 해설할 때 **주변 단어 중 무엇에 집중할지(=Attention** 결정한다.

### Feedforward
트랜스포머 내부에는 **Feedforward Network**도 포함되어 있다.
이는 모델이 더 많은 언어 패턴을 저장하고 학습할 수 있도록 돕는다.

Attention과 Feedforward 연산을 여러 층(layer)에 걸쳐 반복하면,    
모델은 각 단어의 의미를 점점 더 정교하게 이해하고, 마지막 단계에서는 전체 문맥을 반영한 확률 분포로 다음 단어를 예측한다.
