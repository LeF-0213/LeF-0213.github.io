---
layout: post
related_posts:
  - /aiLog/ai
title:  "자연어 처리(NLP, Natural Language Processing)"
date:   2026-01-12
categories:
  - ai
description: >
  
---
* toc
{:toc .large-only}

## 자연어 (Natural Language)

자연어는 인간이 일상적으로 의사소통에 사용하는 언어로, 말과 글을 통해 표현되는 언어를 말한다.           
자연어는 문법, 어휘, 맥락, 뉘앙스 등 복잡한 요소들로 이루어져 있어 규칙적인 구조와 함께 다양한 변형이 가능하다.         
컴퓨터 과학에서는 이러한 자연어를 이해하고 처리하기 위한 자연어 처리(NLP, Natural Language Processing) 기술이 사용되며,         
이를 통해 텍스트 분석, 변역, 음석 인식, 챗봇과 같은 다양한 응용이 가능하다.         
자연어는 인간의 사고와 문화적 배경을 반영하므로, 이를 다루는 기술은 인문학적 이해와 기술적 접근이 결합되어야 한다. (도메인 지식 필요)

## 자연어 처리 (NLP, Natural Language Processing)

자연어 처리는 컴퓨터가 인간의 언어를 이해하고 처리하며 생성할 수 있도록 돕는 인공지능 기술 분야이다.            
이를 통해 텍스트나 음성 데이터를 분석하고, 변역, 요약, 감정 분석, 질의 응답, 음성 인식 등 다양한 작업을 수행할 수 있다.         
자연어 처리는 언어학, 컴퓨터 과학, 인공지능의 융합으로 이루어지며, 형태소 분석, 구문 분석, 의미 분석 등 여러 단계를 포함한다.           
최근에는 딥러닝 기술과 대규모 언어 모델의 발전으로 자연어 처리 선능이 크게 향상되어         
챗봇, 검색 엔진, 추천 시스템과 같은 실생활 응용에서 널리 사용되고 있다.

> 자연어 처리(텍스트)와 컴퓨터 비전(이미지)는 각각 독립된 분야로 연구되었으나,          
> 현재는 멀티모달 방식으로 이 둘을 통합하여 '인간처럼 시각과 언어를 동시에 이해하는 모델'이 대세가 되었다.

## 토큰화 (Tokenization)

토큰화는 텍스트 데이터를 분석하거나 처리하기 위해 문장을 의미있는 단위로 나누는 과정이다. 
이 단위는 단어, 어절, 형태소, 또는 문자 단위일 수 있으며 (상황마다 다름),
자연어 처리(NLP)의 기초 단계로 매우 중요한 역할을 한다.
예를 들어, 영어에서는 공백이나 구두점을 기준으로 단어를 나누는 것이 일반적이지만,
한국어나 일본어처럼 공백이 명확하지 않은 언어에서는 형태소 분석기를 사용하여 어절이나 형소 단위로 나누는 작업이 필요하다.

### 토큰 (Token)

인공지능이 텍스트를 처리할 수 있도록 문장을 잘게 쪼갠 **'최소 처리 단위'**를 의미

### 정수 인코딩 (Integer Encoding)

정수 인코딩은 자연어 처리(NLP)에서 텍스트 데이터를 모델이 이해할 수 있는 숫자 형태로 변환하는 과정이다.         
문장을 토큰화(Tokenization)해 단어(Word)・서브워드(Subword)・문자 단위로 나눈 뒤,           
각 토큰에 고유한 정수 ID를 부여하여 **문장을 숫자 시퀀스로 표현**한다.          
이렇게 정수로 변환된 데이터는 신경망 모델에 입력될 수 있고,             
이후 임베딩 레이어를 통해 연속적인 벡터 공간으로 매핑되어 의미 정보를 학습하는 기반이 된다.

## 단어 사전 (Vocabulary)

### OOV

## 벡터화

### 원-핫 인코딩 (One-Hot Encoding)

원 핫 인코딩은 텍스트 데이터를 숫자 벡터로 변환하는 가장 단순한 방법 중 하나로,             
단어 집합 (vocabulary)에 있는 단어들을 고유한 숫자 벡터로 표현한다.         
이 방법은 각 단어를 벡터의 한 차원으로 지정하고, 해당 단어에만 1을 넣고 나머지는 0으로 설정한다.
이를 통해 텍스트 데이터를 컴퓨터가 이해할 수 있는 숫자로 변환한다.
예를 들어, 단어 집합이 ["I", "love", "Python"] 이라면, 
"love"는 [0, 1, 0] 으로 표현된다.

이 방법은 단어를 간단하고 직관적으로 표현할 수 있지만,


<img src='https://img1.daumcdn.net/thumb/R1280x0/?scode=mtistory2&fname=https%3A%2F%2Fblog.kakaocdn.net%2Fdna%2FA2Aqe%2FbtsLUsDWdb3%2FAAAAAAAAAAAAAAAAAAAAAO2jzzLITF4ijs7zSuMhSQRnYJ1zMM-DB3C5It45GSmw%2Fimg.png%3Fcredential%3DyqXZFxpELC7KVnFOS48ylbz2pIh7yKj8%26expires%3D1769871599%26allow_ip%3D%26allow_referer%3D%26signature%3DA1xCOSt7TsRcxUiUcqEp5F7eidU%253D' />

#### 희소 벡터 (Sparse Vector)

#### 조밀한 벡터 (Dense Vector)

### Bag of Words, Bow

<img src='https://img1.daumcdn.net/thumb/R1280x0/?scode=mtistory2&fname=https%3A%2F%2Fblog.kakaocdn.net%2Fdna%2FkWUmh%2FbtsP01axJzv%2FAAAAAAAAAAAAAAAAAAAAACCzyLBUcdrdlI1f9LVx-k7eqbEpnHDz_QXyGCx-xdwm%2Fimg.png%3Fcredential%3DyqXZFxpELC7KVnFOS48ylbz2pIh7yKj8%26expires%3D1769871599%26allow_ip%3D%26allow_referer%3D%26signature%3DGHTqor3UIGKd%252B7xCFRagWj7gPik%253D' />

Bag of Words(Bow)는 텍스트 데이터를 벡터화하는 

### DTM (Document-Term Matrix, 문서-단어 행렬)

DTM은 여러 문서를 행으로, 단어 사전에 포함된 단어들을 열로 두어 **단어가 몇 번 등장했는지**를 **수치로 표현한 행렬**이다. 즉, **문서들을 단어 기반으로 정리한 표 형태**로서, 각 칸의 값은 단어 빈도나 TF-IDF 값 등이 들어가며, 이를 통해 문서를 수치화하며 머신러닝이나 텍스트 마이닝에서 분류, 군집, 주제 모델링 같은 작업에 활용할 수 있다.

```python

```

```python

```

```python

```

```python

```

## TF-IDF (Term Frequency-Inverse Document Frequency)

TF-IDF는 텍스트 데이터에서 단어의 중요도를 측정하는 방법으로,
단어의 빈도(TF)와 역문서 빈도(IDF)를 조합하여 계산된다.

단어의 빈도(TF, Term Frequency)
역문서 빈도(IDF, Inverse Document Frequecy)

## 코사인 유사도 (Cosine Similarity)

코사인 유사도는 두 벡터 간의 방향(각도)을 기반으로 유사도를 측정하는 방법으로,
벡터 간의 크기가 아니라 방향이 얼마나 유사한지를 나타낸다.
두 벡터의 코사인 유사도는 코사인 함수를 사용해 계산되며, 값의 범위는 **-1에서 1**사이이다.


