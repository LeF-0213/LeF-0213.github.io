---
layout: post
related_posts:
  - /aiLog/ai
title:  "선형회귀"
date:   2025-11-02
categories:
  - ai
description: >
  
---
* toc
{:toc .large-only}

# 선형회귀
데이터에서 관계를 찾다
데이터가 일직선으로 되어있을 때

## 단항 선형 회귀
### 선형 회귀 분석 (Linear Regression)
Y = Wx + b
w(weight) 가중치로 기울기
b(bias) 편향값으로 절편값

손실함수: 예측과 실제의 차이를 수치로 평가해주는 함수

오차값 함이 0이면 -> 데이터와 함수가 100% 일치

MSE =  (실제 - 예측값) ^ 2
손실값의 전체합이 최소가 될 때가지 선형회귀하기

|실제 - 예측|

차이를 제곱하면, 전체 오차합에서 차이가 큰 값의 비중이 너무 커져서 루트해주면, 그 차이를 절대값으로 한 것과 비슷해짐

손실함수는 모델의 예측이 얼마나 틀렸는지 측정하고, 학습 방향을 알려주는 지표이다.
module.mae

최적화
손실함수의 값을 바탕으로 파라미터(w, b) 조절
optimization

## 비선형 함수
Sigmoid
ReLu
TanH

최적화를 조절하는 법
## 손실 함수(Loss Function)
예측과 실제의 차이를 수치로 계산해주는 것이 손실 함수이다.

## 경사하강법(Gradient Descent)
경사하강법(Gradient Descent)은 손실 함수의 기울기(Gradient)를 계산하여 오차가 줄어드는 방향으로 가중치(W)와 편향(b)을 업데이트하는 방법이다.

* **기울기**: 손실이 얼마나, 어느 방향으로 바뀌는지를 나타냄
* **학습률(Learning Rate, lr)**: 한 번에 이동하는 거리(걸음 크기)
* **과정**: 기울기 계산 -> 반대 방향으로 이동 -> 손실 감소

기울기가 0에 가까워질수록 더 이상 내려갈 곳이 없고,
그 지점이 최적점(Optimal  Point)이다.

가중치(w)와 편향값(b)를 조절을 했을 때 손실값(Loss)이 가장 적은 최적의 구간을 찾는것
기울기를 미분을 해서 기울기에 맞게 이동을 시켜 손실값이 최저가 되는 부분을 찾는 것

epoch를 돌린다.
epoch를 100번 돌린다.
1일때, 2일때, 3일때 ... 99때
각각의 Loss와 accuracy를 뽑음

epoch Loss    accuracy
1     13.6    50%
2     10      60%
3     7       86%
50    6.3     94%
66    6.2     94%
99    6.2     94%     

이동하는 것을 학습률이라고 해서 학습률이 적으면 조금씩 이동 학습률이 크면 많이 이동

경사하강법은 결국 기울기를 따라 손실이 감소하는 방향으로 이동

기울기를 계산 -> 반대 방향으로 이동 -> 손실 감소

경사하강법에는 종류가 세가지가 있다.

학습률(Learing rate)이란?
한 번에 얼마나 이동할지 정하는 속도 조절장치

학습률이 많으면 속도가 빠르고
학습률이 작으면 속도가 느림

가중치 업데이트
경사하강법의 결과로 w값을 조절한다.

손실함수 L을 가중치 w에 대해 미분한 값

편향 업데이트
루프 돌릴 때 학습률(에타기호)의 크기를 조절하는 것이 중요

확률적 경사하강법
SGD
경사하강법에서 한자리에서 도는 문제를 해결할 수 있음

Adam이란?
학습률을 자동으로 조절하는 장치 중 하나
관성 + 속도 조절을 하는 최적화 알고리즘

다중 선형 회귀
길이랑 무게뿐 아니라 두께도 추가해 좀 더 정확한 측정
선형 회귀에서 추가적인 조건을 넣어주는 것

y = W1x1 + W2x2 + b
선형회귀는 일차 함수라 복잡한 차원을 계산을 못함

Alexnet
CIFAR

시그모이드(Sigmoid) 함수
출력을 0 ~ 1로 변환

이진 분류

시그모이드 함수를 활용
단항 논리 회귀
공부 시간 X
합격 불합격 Y
x가 오른쪽으로 갈수록 y 증가
커질수록 확률이 높아지는 것

단항 논리 회귀에 쓰이는 함수
BCE 손실함수

다항 논리 회귀

다항 논리 회구에 쓰이는 함수
Softmax 함수
GOT
Generative pre-trained Transformer

CrossEntropyLoss

인공신경망
선형 모델

비선형 모델
인간의 뇌

인공 신경망
뉴런
수상돌기
세포체
축삭
시냅스

입력 -> 가중합 -> 출력
입력층 -> 은닉층 -> 출력층

퍼셉트론
단일 퍼셉트론 이분법으로 분류

이분법으로 분리가 되지 않아
다층 퍼셉트론이 등장

**오차 역전파**
1. 순전파
입력 데이터를 네트워크(은닉층)에 통과시켜서 예측값을 계산
1. 오차 계산
손실함수로 예측값과 실제값의 차이(오차)
1. 역전파(BackPropagation)
체인룰을 이용해
각 층의 가중치가 오차에 얼마나 영향을 줬는지를 계산
1. 가중치 업데이트


비선형 활성화 함수
시그모이드, 소프트맥스, ReLu, TanH

시그모이드는 값이 너무 크면 값이 손실이 일어나는데
ReLu는 값이 커도 보존이 된다.

AI 모델이란?
입력값에 대해 출력값을 학습시텨서
우리가 원하는 출력값을 도출시키는 아이

클래스
CrossEntropyLoss
softmax 함수의 손실 함수

## Adam
